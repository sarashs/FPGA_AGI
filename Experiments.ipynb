{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import deque\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "import pprint\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from FPGA_AGI.chains import WebsearchCleaner\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "\n",
    "# import importlib\n",
    "from FPGA_AGI import tools\n",
    "from FPGA_AGI import parameters\n",
    "from FPGA_AGI import utils\n",
    "from FPGA_AGI import prompts\n",
    "from FPGA_AGI import chains\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from FPGA_AGI import agents\n",
    "from FPGA_AGI.agents import Engineer\n",
    "from FPGA_AGI import utils\n",
    "from FPGA_AGI.utils import plot_graph\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "#os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)\n",
    "bigllm = ChatOpenAI(model='gpt-4-turbo', temperature=0)\n",
    "gpt4 = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "gpt4o = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "# Improve the h file generation\n",
    "# Get human objective\n",
    "# refine it via human in the loop and via adding context (short/long term memory search) -> the output is a json dict of the design\n",
    "# build the modules and test benches usiiing short and long term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcleaner = WebsearchCleaner.from_llm(llm=gpt4o)\n",
    "\n",
    "def clean_web(concatenated_content):\n",
    "    try:\n",
    "        cleaned_concatenated_content = webcleaner.invoke(concatenated_content)\n",
    "        cleaned_concatenated_content = cleaned_concatenated_content.cleaned\n",
    "    except:\n",
    "        cleaned_concatenated_content = concatenated_content.replace('\\n\\n', '')\n",
    "    return cleaned_concatenated_content\n",
    "\n",
    "def context_from_web(url):\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Add \n",
    "    #docs.extend([*docs_pydantic, *docs_sq])\n",
    "\n",
    "    # Sort the list based on the URLs in 'metadata' -> 'source'\n",
    "    d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "    d_reversed = list(reversed(d_sorted))\n",
    "\n",
    "    # Concatenate the 'page_content' of each sorted dictionary\n",
    "    concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "        [doc.page_content for doc in d_reversed]\n",
    "    )\n",
    "    cleaned_concatenated_content = clean_web(concatenated_content)\n",
    "    return cleaned_concatenated_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "We have a helper chain that generates requirements from an initial prompt. This helper function may or may not have been mentioned in the paper.\n",
    "An alternative way to prepare them would be the following:\n",
    "```python\n",
    "from FPGA_AGI.chains import Requirements\n",
    "\n",
    "project_requirements = Requirements()\n",
    "project_requirements.goals = [list_of_goals]\n",
    "project_requirements.requirements = [list_of_requirements]\n",
    "```\n",
    "\n",
    "## Simple RISC V CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FPGA_AGI.chains import RequirementChain\n",
    "\n",
    "requirement_chain = RequirementChain.from_llm(gpt4o)\n",
    "\n",
    "rescv_concatenated_content = context_from_web(\"\"\"https://www.fpga4student.com/2017/04/verilog-code-for-16-bit-risc-processor.html\"\"\")\n",
    "\n",
    "riscv_requirements = requirement_chain.invoke(\n",
    "    {\"objective\": \"\"\"Design a simple educational RISC-V CPU. Make sure that your code is synthesizable and can aesily fit on a zynq7 device. The coding language must be hls c++. The code must be supplied with a lot of comments for educational purposes.\"\"\", \n",
    "     \"context\" : rescv_concatenated_content}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_concatenated_content = context_from_web(\"\"\"https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm\"\"\")\n",
    "\n",
    "fft_requirements = requirement_chain.invoke(\n",
    "    {\"objective\": \"\"\"Build a 128 point fft circuit which is using two 64 point fft modules (named fft64) to compute the 128 point fft. You do not need to design the 64 point fft devices. You can assume that they just exist.\n",
    "     the input to the fft module is an array of 128 double precision fixed point real numbers (such as a DSP signal measured elsewhere.) The implementation language must be HLS C++. The design must be optimized for maximum performance (speed.)\n",
    "     The design must be commented with comments indicating where the design is in fact optimized for performance.\"\"\", \n",
    "     \"context\" : fft_concatenated_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadruple precision floating point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float128_concatenated_content = context_from_web(\"\"\"https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format\"\"\")\n",
    "\n",
    "float128_requirements = requirement_chain.invoke(\n",
    "    {\"objective\": \"\"\"\n",
    "     Build a quadrouple precision floating point (not the fixed point) exponentiation module. You have to code this 128 bit floating point representation from scratch, using the context I provided, and code the computations based on this representation.\"\"\", \n",
    "     \"context\" : float128_concatenated_content}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge base (vector database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "\n",
    "embeddings_3_small = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "if os.path.isdir('knowledge_base'):\n",
    "    persistent_client = chromadb.PersistentClient(path=\"./knowledge_base\")\n",
    "    pdfsearch = Chroma(client=persistent_client, embedding_function=embeddings_3_small)\n",
    "else:\n",
    "    pages = []\n",
    "    for item in [\"https://arxiv.org/pdf/1502.07055\", \"https://arxiv.org/pdf/1810.06885\", \"https://arxiv.org/pdf/1808.02521\", \"https://riscv.org/wp-content/uploads/2017/05/riscv-spec-v2.2.pdf\"]: # \n",
    "        loader = PyPDFLoader(item, extract_images=True) #\"https://riscv.org/wp-content/uploads/2017/05/riscv-spec-v2.2.pdf\"\n",
    "        pages += loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(pages)\n",
    "    pdfsearch = Chroma.from_documents(texts, embeddings_3_small, collection_name= \"knowledge_base\", persist_directory=\"./knowledge_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All test runs as per the article\n",
    "Each test consists of:\n",
    "- requirements: project goals and requirements\n",
    "- input_context: Project specific informations\n",
    "- knowledge base (a vector database of all of the necessary information including textbooks and codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review = None\n",
    "for requirements, input_context, name in zip(\n",
    "    [riscv_requirements, fft_requirements, float128_requirements],\n",
    "    [rescv_concatenated_content, fft_concatenated_content, float128_concatenated_content],\n",
    "    ['Risc-v', 'FFT', 'Float-128']):\n",
    "    review = None\n",
    "    for eval_mod in [bigllm, gpt4o]:\n",
    "        for model in [llm, bigllm, gpt4o]:\n",
    "            for i in range(2):\n",
    "                sol_dir = f'{name}_{model.model_name}_evaluation_{eval_mod.model_name}_{i}'\n",
    "                R = Engineer(model=model, evaluator_model=eval_mod, retriever=pdfsearch.as_retriever(search_kwargs={\"k\": 1}), solution_num=sol_dir)\n",
    "                try:\n",
    "                    if review:\n",
    "                        R.lit_review_results = review\n",
    "                    R.invoke(goals=requirements.goals, requirements=requirements.requirements, input_context= input_context)\n",
    "                    with open(f\"solution_{sol_dir}/goals_requirements.txt\", \"w\") as file:\n",
    "                        file.write(\"Goals: \\n\" + '\\n'.join(requirements.goals) + \"\\nRequirements: \\n\" + '\\n'.join(requirements.requirements))\n",
    "                    plot_graph(R.hierarchical_solution_result, save_path=f\"solution_{sol_dir}/graph.png\")\n",
    "                except:\n",
    "                    pass\n",
    "                review = R.lit_review_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
